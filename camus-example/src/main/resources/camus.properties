# Needed Camus properties, more cleanup to come

# final top-level data output directory, sub-directory will be dynamically created for each topic pulled
etl.destination.path=/data/acep/camus-test/events

# HDFS location where you want to keep execution files, i.e. offsets, error logs, and count files
etl.execution.base.path=/data/acep/camus-test/events/output
# where completed Camus job output directories are kept, usually a sub-dir in the base.path
etl.execution.history.path=/data/acep/camus-test/events/complete

# PROD
#zookeeper.hosts=cdp-sleuth-kafka-01.cdp.webapps.rr.com:2181
zookeeper.hosts=dnvrco01-hp-evg0001.conops.timewarnercable.com:2181,dnvrco01-hp-evg0002.conops.timewarnercable.com:2181,dnvrco01-hp-evg0003.conops.timewarnercable.com:2181
# STAGING
#zookeeper.hosts=cdp-eventgw-09.cdp.webapps.rr.com:2181
#zookeeper.hosts=localhost:2181
zookeeper.broker.topics=/brokers/topics
zookeeper.broker.nodes=/brokers/ids

# Concrete implementation of the Encoder class to use (used by Kafka Audit, and thus optional for now)
#camus.message.encoder.class=com.linkedin.batch.etl.kafka.coders.DummyKafkaMessageEncoder

# Concrete implementation of the Decoder class to use
camus.message.decoder.class=com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder
#camus.message.decoder.class=com.linkedin.batch.etl.kafka.coders.TWCEnrichedEventMessageDecoder
# Used by avro-based Decoders to use as their Schema Registry
kafka.message.coder.schema.registry.class=com.linkedin.camus.schemaregistry.EnrichedEventSchemaRegistry

# Used by the committer to arrange .avro files into a partitioned scheme. This will be the default partitioner for all
# topic that do not have a partitioner specified
etl.partitioner.class=com.linkedin.camus.etl.kafka.coders.DefaultPartitioner

# Partitioners can also be set on a per-topic basis
#etl.partitioner.class.<topic-name>=com.your.custom.CustomPartitioner

# all files in this dir will be added to the distributed cache and placed on the classpath for hadoop tasks
hdfs.default.classpath.dir=/etc/hadoop/conf

# max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=1
# max historical time that will be pulled from each partition based on event timestamp
#kafka.max.pull.hrs=1
# events with a timestamp older than this will be discarded. 
kafka.max.historical.days=3
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=-1

# if whitelist has values, only whitelisted topic are pulled.  nothing on the blacklist is pulled
kafka.blacklist.topics=

#kafka.whitelist.topics=big_data_v0.3.131-ebe5a7,vodbeta_v2,channelbeta_v2
kafka.whitelist.topics=prod-eg_v2_2-big_data_v0.3.256-68d94,vodbeta_v1,channelbeta_v1

# Name of the client as seen by kafka
kafka.client.name=camus

# Fetch buffer size for Kafka
kafka.fetch.buffer.size=20971520

# Correlation Id for fetch requests. Ideally should be an incrementing integer with every request
kafka.fetch.request.correlationid=-1

# Max wait for fetch requests
kafka.fetch.request.max.wait=60000

# Min bytes needed for fetch requests
kafka.fetch.request.min.bytes=4096

# Main VIP for Kafka brokers
# PROD
#kafka.host.url=cdp-sleuth-kafka-01.cdp.webapps.rr.com
kafka.host.url=dnvrco01-hp-kfa0001.conops.timewarnercable.com
#kafka.host.url=71.74.46.28
# STAGING
#kafka.host.url=cdp-eventgw-09.cdp.webapps.rr.com
#kafka.host.url=localhost
kafka.host.port=9092

# Timeout value for Kafka Connections
kafka.timeout.value=60000
kafka.move.to.last.offset.list=

#Stops the mapper from getting inundated with Decoder exceptions for the same topic
#Default value is set to 10
max.decoder.exceptions.to.print=5

#Controls the submitting of counts to Kafka
#Default value set to true
post.tracking.counts.to.kafka=true

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily
etl.ignore.schema.errors=false

# configure output compression for deflate or snappy. Defaults to deflate
etl.output.codec=deflate
etl.deflate.level=6
#etl.output.codec=snappy

etl.default.timezone=America/New_York
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

mapred.output.compress=true
mapred.map.max.attempts=1

kafka.client.buffer.size=20971520
kafka.client.so.timeout=60000

#zookeeper.session.timeout=
zookeeper.connection.timeout=60000
